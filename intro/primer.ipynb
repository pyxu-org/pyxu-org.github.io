{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048ab4a4-ba09-44d2-8edb-edef60ffd1b4",
   "metadata": {},
   "source": [
    "# Computational Imaging in a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a3355-7bb5-481d-878c-e69f4d264e10",
   "metadata": {},
   "source": [
    "## The Multifaceted Realm of Computational Imaging\n",
    "Digital image restoration and enhancement techniques—concisely termed as image reconstruction methods—serve as the linchpins in the evolving landscape of computer vision. These techniques endeavor to rejuvenate degraded or partially captured raw images, transforming them into superior-quality versions better aligned for visualization or intricate image analysis. The resultant images often boast enhanced resolution and perceptual quality, diminished noise and blur, and may even feature completions in areas where data is lacking. \n",
    "\n",
    "At the core of many image reconstruction techniques lies the challenge of solving a mathematical inverse problem. Here's how it works: an object of interest—be it a cell, a scene, or something else—is observed through an acquisition system, such as a microscope or camera. This system collects data that is often both noisy and blurred. The ultimate objective is to reconstruct the original object from these imperfect measurements, effectively \"inverting\" the acquisition process to obtain a clearer image or representation. The diagram below succinctly illustrates an inverse problem setting:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc0751b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img align=\"center\" src=\"../_static/tutorial/g10575.png\" alt=\"Forward model\" width=70%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a873dac-49d0-4297-b775-b482b89e73c0",
   "metadata": {},
   "source": [
    "where: \n",
    "\n",
    "* $f$ denotes the unknown image, \n",
    "* $\\Phi$ is an operator modelling the acquisition system (typically linear),\n",
    "* $n$ is some random noise, typically additive and zero-mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9970cd46-33e8-4565-8e42-b6cdc8069004",
   "metadata": {},
   "source": [
    "Common examples of computational imaging tasks include for example:\n",
    "\n",
    "* **Image Denoising:** The process of eliminating noise artifacts to create a cleaner, crisper image.\n",
    "* **Image Deblurring:** Restoration of a sharp image from a blurry input, enhancing focus and detail.\n",
    "* **Image Inpainting:** Reconstructing missing or damaged regions within an image, often used for tasks like replacing lost blocks during coding and transmission or erasing watermark/logo overlays.\n",
    "* **Image Super-Resolution:** Elevating the resolution of an image or an imaging system to provide finer detail.\n",
    "* **Image Fusion:** The merging of two or more degraded images of the same object or scene into a single image that exceeds the quality of any individual input.\n",
    "* **Image Filtering:** Modifying an image to accentuate particular features of interest, such as points, lines, or shapes.\n",
    "* **Tomographic Reconstruction:** Rebuilding an image from its lower-dimensional projections, known as sinograms in the context of CT or PET scans.\n",
    "* and many more..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a1b5f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> \n",
    "<img align=\"center\" src=\"../_static/tutorial/recon_examples.jpg\" alt=\"Forward model\" width=95%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a0c0c-ca29-4738-875f-36c76cc5e689",
   "metadata": {},
   "source": [
    "## The Dawn of Computational Imaging\n",
    "\n",
    "Traditional software-based imaging pipelines have often relied on direct inversion techniques. These methods provide a rough approximation of the pseudoinverse of the sensing operator that models the image acquisition system. While these approaches are fast, intuitive, and relatively scalable, they are fundamentally limited in terms of accuracy, frequently resulting in poorly resolved images marred by significant reconstruction artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4cf477",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img align=\"center\" src=\"../_static/tutorial/illposed.jpg\" alt=\"Forward model\" width=70%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10527730-083e-4c9d-96c3-f9f39237ccc8",
   "metadata": {},
   "source": [
    "One way to counteract the numerical instability inherent in the use of the pseudo-inverse is by solving dampened normal equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcfa2ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img align=\"center\" src=\"../_static/tutorial/g15273.png\" alt=\"Forward model\" width=70%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0395b067-5e68-4afc-8528-2ae97c928170",
   "metadata": {},
   "source": [
    "However, a key limitation of such conventional reconstruction schemes is their fixed-function design. This rigidity prevents the incorporation of any prior knowledge about the observed scene, such as its smoothness, sparsity, or high compressibility in certain domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8adcf7-91c9-496f-845f-57237a8bc98b",
   "metadata": {},
   "source": [
    "## Bayesian Computational Imaging\n",
    "\n",
    "State-of-the-art image restoration techniques generally rely on powerful and universal image priors that promote specific perceptual or structural features commonly found in natural images. These effectively regularise the ill-posed inverse problem and improve reconstruction accuracy. This is typically achieved by means of handcrafted Bayesian estimation problems, built from the composition of universal mathematical building blocks (e.g., mathematical transforms and cost/regularisation functionals). These Bayesian problems assess an optimal trade-off between a likelihood term, which controls the reconstruction fidelity in terms of the data and its statistics, and a prior term, which promotes physical plausibility of the reconstruction. These Bayesian problems are generally solved using sophisticated iterative first-order proximal-splitting methods and are well-suited for uncertainty quantification through Markov Chain Monte Carlo (MCMC) samplers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4959be3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img align=\"center\" src=\"../_static/tutorial/g15005.png\" alt=\"Forward model\" width=70%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9651af-7a94-460b-8708-0cd80a9fad83",
   "metadata": {},
   "source": [
    "At their core, image priors aim to approximate the intricate and high-dimensional manifold of natural images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d32f25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img align=\"center\" src=\"../_static/tutorial/manifold.jpg\" alt=\"Forward model\" width=95%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47692d11-39db-42b0-b868-b92459e3bb2a",
   "metadata": {},
   "source": [
    "Historically, this approximation was often achieved using methods like Tikhonov or Total Variation priors. These traditional priors emphasized simple, physically-admissible behaviors such as smoothness or sparsity within a specific domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f214065b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img align=\"center\" src=\"../_static/tutorial/l2_l1.jpg\" alt=\"Forward model\" width=95%>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51e406c9-bc32-4bb0-856f-f4968040adaf",
   "metadata": {},
   "source": [
    "## The Deep Learning Revolution\n",
    "\n",
    "The rise of deep learning has been a game-changer in the field of computational imaging, resulting in a new era of highly effective, data-driven image priors. These advanced priors are capable of learning the manifold of natural images much more fidely than model-based priors. Consequently, contemporary techniques for image restoration, enhancement, and manipulation have reached unprecedented levels of performance. In fact, the advancements have been so significant that some experts speculate we may have arrived at a performance and accuracy plateau:\n",
    "\n",
    "> Recent studies suggest that modern image denoising methods are approaching the optimally possible performance in denoising. (Romano et al., 2017)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db93108",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T07:26:36.218288Z",
     "start_time": "2023-05-12T07:26:36.213864Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img align=\"center\" src=\"../_static/tutorial/nerf_dark.gif\" alt=\"Forward model\" width=70%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a43ed-41c5-4e30-b7a9-c95141080ea0",
   "metadata": {},
   "source": [
    "## The Adoptability and Usability Crisis of Computational Imaging\n",
    "Despite the remarkable strides made in the field, advanced image reconstruction technologies face significant obstacles in terms of adoptability, usability, and reproducibility, particularly in applied imaging sciences. A large portion of the computational imaging techniques cited in academic literature are hyper-specialized for specific applications, and many remain in the proof-of-concept stage. These methods often necessitate expert knowledge to calibrate, utilize, and integrate into production pipelines. Further, many lack support for N-dimensional images, distributed out-of-core computing, and hardware acceleration, which restricts their practical applicability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3fa591-489b-4f47-8948-cac17dcc28f8",
   "metadata": {},
   "source": [
    "To accelerate the path from research prototyping to production deployment in imaging science, there is hence a strong need to rethink traditional imaging pipelines, with an emphasis on scalability (for both CPUs and GPUs) and modularity (to allow for high customizability and coexistence of both standard and advanced techniques). This requires in particular to transition from monolithic to microservice ones with highly maintainable, testable, optimised, loosely coupled, hardware-agnostic and universal software components. Such a design of the imaging pipeline should enable the rapid and reliable prototyping/scaling/deployment of complex image reconstruction methods and deliver on their many promises beyond academic environments. This core objective is, in essence, the very *raison d'être* of Pyxu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef83cd3-e13d-4c61-a8cb-334c5256445f",
   "metadata": {},
   "source": [
    "## Pyxu: High-Performance Computational Imaging with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b847a8-ac20-419e-ac87-8401c860f200",
   "metadata": {},
   "source": [
    "Pyxu is an open-source computational imaging software framework for Python with native support for hardware acceleration and distributed computing. The latter adopts a modular and interoperable microservice architecture providing highly optimised and scalable general-purpose computational imaging functionalities and tools, easy to reuse and share across imaging modalities. These include notably a rich collection of common sensing operators and cost/penalty functionals, which can easily be combined via an advanced operator algebra logic to handcraft tailored inverse problems mappable to the generic state-of-the-art (stochastic) proximal methods of the library’s comprehensive algorithmic suite. This suite simplifies the user experience, automatically calculating gradients, proximal operators, and hyperparameters through the operator algebra logic or guided by best practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccca626-2208-4615-a1c8-4fb3f944619c",
   "metadata": {},
   "source": [
    "At its foundation, Pyxu takes advantage of an array of high-performance computing (HPC) tools from the Python ecosystem to assure superior performance and scalability. For instance, Pyxu's computational logic operates across multiple precisions and fully supports Duck arrays—array-like entities compliant with the NumPy API standard. This compatibility encompasses not just traditional CPU-bound NumPy arrays, but also extends to GPU-bound CuPy arrays and distributed or out-of-core chunked Dask arrays. Internally, the framework employs a module-agnostic codebase, dispatching routine calls to either NumPy, CuPy, Dask, or any other NumPy-compatible library based on the type of array in operation. As a result, Pyxu offers native support for both hardware acceleration and distributed computing within a singular, easily maintainable, testable, and readable Python codebase."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "rise": {
   "footer": "Matthieu Simeoni, EPFL Hub for Advanced Image Reconstruction (AIR)",
   "progress": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
