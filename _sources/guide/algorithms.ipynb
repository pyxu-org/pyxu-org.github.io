{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c075995b-e285-4cbc-a14d-86231cc4e19c",
   "metadata": {},
   "source": [
    "# Optimization Algorithms\n",
    "\n",
    "Navigating the landscape of optimization algorithms can be overwhelming at times, especially when each algorithm comes with its own set of assumptions and applicability scope. Pyxu aims to simplify this by offering a comprehensive suite of algorithms tailored for solving Bayesian estimation problems. In this section, we'll walk through the different algorithms available, discuss how to choose the most suitable ones for your problem, and show you how to flexibly configure them. 🎛️\n",
    "\n",
    "## Algorithms Overview\n",
    "\n",
    "### Conjugate Gradient (CG) [🔗](../api/opt.solver.html#pyxu.opt.solver.CG)\n",
    "👉 **Use case:** Quadratic problems, particularly when you have a well-conditioned matrix $A$.\n",
    "\n",
    "**Mathematical Form:**\n",
    "$$\n",
    "\\min_{x\\in\\mathbb{R}^{N}} \\frac{1}{2} \\mathbf{x}^{T} \\mathbf{A} \\mathbf{x} - \\mathbf{x}^{T} \\mathbf{b}\n",
    "$$\n",
    "\n",
    "### Accelerated Proximal Gradient Descent (APGD)  [🔗](../api/opt.solver.html#pyxu.opt.solver.PGD)\n",
    "👉 **Use case:** Problems that separate into smooth and non-smooth components. \n",
    "\n",
    "**Mathematical Form:**\n",
    "$$\n",
    "\\min_{\\mathbf{x}\\in\\mathbb{R}^N} \\;\\mathcal{F}(\\mathbf{x})\\;\\;+\\;\\;\\mathcal{G}(\\mathbf{x}),\n",
    "$$\n",
    "where $\\mathcal{F}$ and $\\mathcal{G}$ are differentiable and proximable functionals, respectively.\n",
    "\n",
    "> **Note:** With acceleration, `APGD` can be shown to be an *optimal* first-order method, with the fastest possible convergence rate!\n",
    "\n",
    "\n",
    "### Primal-Dual Splitting (PDS) Methods \n",
    "👉 **Use case:** Multi-term problems, with smooth and non-smooth terms, possibly composed with a linear operator $K$.\n",
    "\n",
    "**Mathematical Form:**\n",
    "$$\n",
    "\\min_{\\mathbf{x}\\in\\mathbb{R}^N} \\;\\mathcal{F}(\\mathbf{x})\\;\\;+\\;\\;\\mathcal{G}(\\mathbf{x})\\;\\;+\\;\\;\\mathcal{H}(K\\mathbf{x})\n",
    "$$\n",
    "where $\\mathcal{F}$ is differentiable, $\\mathcal{G}$ and $\\mathcal{H}$ are proximable, and $K$ is a linear operator.\n",
    "\n",
    "Methods under this category include `CondatVu` [🔗](../api/opt.solver.html#pyxu.opt.solver.CondatVu), `PD3O` [🔗](../api/opt.solver.html#pyxu.opt.solver.PD3O), `ADMM` [🔗](../api/opt.solver.html#pyxu.opt.solver.ADMM), `ChambollePock` [🔗](../api/opt.solver.html#pyxu.opt.solver.ChambollePock), and more.\n",
    "\n",
    "Note that, although implemented for three-term objective functionals, PDS methods can easilybe generalized to objective functionals of the form:\n",
    "$${\\min_{\\mathbf{x}\\in\\mathbb{R}^N} \\;\\mathcal{F}(\\mathbf{x})\\;\\;+\\;\\;\\mathcal{G}(\\mathbf{x})\\;\\;+\\;\\;\\sum_{i=1}^J\\mathcal{H}_i(K_i\\mathbf{x})}$$\n",
    "by means of stacking operators:\n",
    "\n",
    "```python\n",
    ">> K = vstack([K_1, ..., K_J])\n",
    ">> H = hstack([h_1, ..., h_J])\n",
    "```\n",
    "\n",
    "## Choosing the Right Algorithm\n",
    "\n",
    "The golden rule is to **choose the most specific algorithm** —i.e., the one that makes the most assumptions consistent with your problem. This often results in faster convergence. For example, if your objective functional has a gradient, a gradient-based method like `APGD` will generally be more efficient than a generic proximal-based method like `DouglasRachford` [🔗](../api/opt.solver.html#pyxu.opt.solver.DouglasRachford).\n",
    "\n",
    "The most generic algorithms in Pyxu are `PD3O` and `CondatVu`, but they are also the least efficient, so use them only when simpler methods like `ADMM`, `APGD` or `CG` cannot be used. Note that `ADAM` [🔗](../api/opt.solver.html#pyxu.opt.solver.Adam) can also be useful when step sizes are too complex to compute. \n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "Pyxu comes with pre-implemented automatic tuning strategies for various algorithms. For instance, the primal-dual splitting methods offer three strategies:\n",
    "\n",
    "1. ``tuning_strategy == 1``: **safe step sizes and no relaxation**.This is the most standard way of setting the parameters in the literature, does not leverage relaxation.\n",
    "2. ``tuning_strategy == 2``: **large step sizes and no relaxation**. This strategy favours large step sizes forbidding the use of overrelaxation. \n",
    "3. ``tuning_strategy == 3``: **safe step sizes and large overrelaxation**. This strategy chooses smaller step sizes, but performs overrelaxation (momentum acceleration).\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "Here's how you can solve a problem involving multiple terms:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{x}\\in\\mathbb{R}^N}\\frac{1}{2}\\left\\|\\mathbf{y}-\\mathbf{A}\\mathbf{x}\\right\\|_2^2\\quad+\\quad\\lambda_1 \\|\\mathbf{D}\\mathbf{x}\\|_1\\quad+\\quad\\lambda_2 \\|\\mathbf{x}\\|_1\n",
    "$$\n",
    "\n",
    "This problem can be written in the form\n",
    "\n",
    "$${\\min_{\\mathbf{x}\\in\\mathbb{R}^N} \\;\\mathcal{F}(\\mathbf{x})\\;\\;+\\;\\;\\mathcal{G}(\\mathbf{x})\\;\\;+\\;\\;\\mathcal{H}(\\mathbf{K} \\mathbf{x})}\n",
    "$$\n",
    "\n",
    "by choosing\n",
    "$\\mathcal{F}(\\mathbf{x})= \\frac{1}{2}\\left\\|\\mathbf{y}-\\mathbf{A}\\mathbf{x}\\right\\|_2^2$,\n",
    "$\\mathcal{G}(\\mathbf{x})=\\lambda_2\\|\\mathbf{x}\\|_1$,\n",
    "$\\mathcal{H}(\\mathbf{x})=\\lambda_1 \\|\\mathbf{x}\\|_1$ and\n",
    "$K=\\mathbf{D}$.\n",
    "\n",
    "Solving this problem with Pyxu amounts to the following steps: \n",
    "\n",
    "```python\n",
    "# Define operators A and D,\n",
    "...\n",
    "\n",
    "# Defining functionals \n",
    "l22_loss = (1 / 2) * SquaredL2Norm(dim=A.shape[0]).asloss(y) * A # Least-squares loss\n",
    "l1_norm = 0.1 * L1Norm(dim=D.shape[0]) # Proximable term H\n",
    "l1_tv = 0.01 * L1Norm(dim=A.shape[1]) # Proximable term F\n",
    "\n",
    "# Initialize solver (Using Condat-Vu as an example)\n",
    "solver = CondatVu(f=loss, g=l1_norm, h=l1_tv, K=D, show_progress=False, verbosity=100)\n",
    "\n",
    "# Fit the model  \n",
    "solver.fit(x0=x0, tuning_strategy=2)\n",
    "sol = solver.solution().squeeze()\n",
    "```\n",
    "\n",
    "## Advanced Usage: Guru Interface \n",
    "\n",
    "For those who want even more control, we provide a guru interface allowing you to overload default settings, including the stopping criteria (see module `pyxu.opt.stop`[🔗](../api/opt.stop.html) for available stopping criteria).\n",
    "\n",
    "For example, overloading the default stopping criterion of the `CondatVu` solver initialized above can be achieved as follows:\n",
    "\n",
    "```python\n",
    "# Custom stopping criterion (optional)\n",
    "custom_stop_crit = (RelError(eps=1e-3, var=\"x\", f=None, norm=2, satisfy_all=True) &\n",
    "                    RelError(eps=1e-3, var=\"z\", f=None, norm=2, satisfy_all=True) &\n",
    "                    MaxIter(20)) | MaxIter(1000)\n",
    "\n",
    "# Fit the model  with the new stopping criterion\n",
    "solver.fit(x0=x0, tuning_strategy=2, stop_crit=custom_stop_crit)\n",
    "```\n",
    "\n",
    "## Implementing New Algorithms \n",
    "\n",
    "To implement a new iterative solver, users need to sub-class `pyxu.abc.solver.Solver`[🔗](../api/abc.html#pyxu.abc.Solver) and overwrite some of its core methods, such as `m_init()`[🔗](../api/abc.html#pyxu.abc.Solver.m_init),  and `m_step()`[🔗](../api/abc.html#pyxu.abc.Solver.m_step), which describe the initalization and update step of iterative algorithms (see the API Reference for more details). \n",
    "\n",
    "\n",
    "Sub-classes of `Solver` inherit automatically from its very versatile API for solving optimisation problems, with the following notable features:\n",
    "\n",
    " * manual/automatic/background execution of solver iterations via parameters provided to `fit()`[🔗](../api/abc.html#pyxu.abc.Solver.fit). \n",
    " * automatic checkpointing of solver progress, providing a safe restore point in case of faulty numerical code. Each solver instance backs its state and final output to a folder on disk for post-analysis. In particular `fit()` will never crash: detailed exception information will always be available in a logfile for post-analysis.\n",
    " * solve for multiple initial points in parallel.\n",
    "\n",
    "Now that you're equipped with the algorithmic know-how, go ahead and choose the best algorithm for your Bayesian estimation problem. Happy optimizing! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
