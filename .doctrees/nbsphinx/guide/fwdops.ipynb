{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec2d8d78-8040-4502-b343-a106b93ac6c3",
   "metadata": {},
   "source": [
    "# Forward Operators\n",
    "\n",
    "In computational imaging, the ability to accurately model the acquisition system is cornerstone to solving inverse\n",
    "problems. The `LinOp()`[üîó](../api/abc.html#pyxu.abc.LinOp) class in Pyxu serves as a foundational abstraction for\n",
    "defining linear forward operators, essentially mapping the original signal or image to the data we observe. This guide\n",
    "distills the knowledge needed to implement custom forward operators using the `LinOp()` class, as well as related\n",
    "classes in the Pyxu library.\n",
    "\n",
    "> **Important Note**: `LinOp` and its derivatives are designed as abstract base classes. This means they serve\n",
    "> as templates or \"blueprints\" for creating specific operators that suit your project needs. You're not supposed to\n",
    "> instantiate them directly. Instead, you have two main options:\n",
    "> \n",
    "> 1. **Subclassing**: Extend these classes to create customized operators tailored to your problem. \n",
    "> \n",
    "> 2. **Generic Constructor Routine**: Utilize the `from_source`[üîó](../api/operator.interop.html#general) function to\n",
    ">    define new operators from their core methods, like `apply()`[üîó](../api/abc.html#pyxu.abc.Map.apply),\n",
    ">    `adjoint()`[üîó](../api/abc.html#pyxu.abc.LinOp.adjoint), or\n",
    ">    `jacobian()`[üîó](../api/abc.html#pyxu.abc.DiffMap.jacobian).\n",
    ">\n",
    "> Additionally, don't forget to explore our comprehensive [Reference API](../api/index.html). It features\n",
    "> pre-implemented versions of many commonly used forward operators, serving as both a shortcut for common tasks and a\n",
    "> useful learning resource.\n",
    "\n",
    "## The Importance of Matrix-Free Operators\n",
    "\n",
    "Computational imaging often involves working with (very) large datasets which poses challenges in terms of scalability.\n",
    "In Pyxu, the strategy for addressing such problems is to employ **matrix-free operators**. These operators are defined\n",
    "implicitly via *methods* rather than explicitly via *matrices*. This strategy saves memory and improves computational\n",
    "efficiency.\n",
    "\n",
    "For example, the following code snippet implements a matrix-free sum operator operating over 1D inputs:\n",
    "\n",
    "```python\n",
    "class Sum(LinOp):\n",
    "    def apply(self, arr):\n",
    "        return arr.sum(keepdims=True)\n",
    "    \n",
    "    def adjoint(self, arr):\n",
    "        return arr * np.ones(self.dim_shape)\n",
    "```\n",
    "\n",
    "This code snippet demonstrates how to define a simple matrix-free linear operator that computes the sum of elements in\n",
    "an array. Let's break down the components:\n",
    "\n",
    "- **Class Definition:**\n",
    "  \n",
    "  ```python\n",
    "  class Sum(LinOp):\n",
    "      pass\n",
    "  ```\n",
    "  Here, a new class called `Sum()` is defined, inheriting from Pyxu's `LinOp()`[üîó](../api/abc.html#pyxu.abc.LinOp) base\n",
    "  class, the primary abstraction for designing real-valued linear operators. This inheritance means that `Sum()` is\n",
    "  expected to implement certain core methods, namely `apply()`[üîó](../api/abc.html#pyxu.abc.Map.apply) and\n",
    "  `adjoint()`[üîó](../api/abc.html#pyxu.abc.LinOp.adjoint). \n",
    "\n",
    "- **The** ``apply()`` **Method:**\n",
    "  \n",
    "  ```python\n",
    "  def apply(self, arr):\n",
    "      return arr.sum(keepdims=True)\n",
    "  ```\n",
    "  The `apply()`[üîó](../api/abc.html#pyxu.abc.Map.apply) method takes an array `arr` as input and returns the sum of its\n",
    "  elements. This is the core functionality of the operator. Instead of representing the sum operation as a matrix and\n",
    "  performing matrix-vector multiplication, the method directly computes the sum, making it \"matrix-free.\"\n",
    "\n",
    "  > **Note**: `keepdims=True` is required since, as per the `LinOp.apply()` signature, it must return an `NDArray`, and\n",
    "  > not a scalar.\n",
    "\n",
    "- **The** ``adjoint()`` **Method:**\n",
    "  \n",
    "  ```python\n",
    "  def adjoint(self, arr):\n",
    "      return arr * np.ones(self.shape[1])\n",
    "  ```\n",
    "  The `adjoint()`[üîó](../api/abc.html#pyxu.abc.LinOp.adjoint) method also takes an array `arr` as input. The adjoint\n",
    "  operation for this sum operator is effectively a *broadcast* operation, mapping a scalar to a vector with the same\n",
    "  dimension of the original array space.\n",
    "\n",
    "\n",
    "## Demystifying the Adjoint\n",
    "\n",
    "The concept of an *adjoint* operator can be elusive, so let's use some intuition to understand it. When we talk about a\n",
    "forward operator in computational imaging or signal processing, we're usually describing a transformation that takes an\n",
    "input (often some form of data or signal) and produces an output (often some form of measurement or transformed data).\n",
    "\n",
    "Now, what if you wanted to go in the opposite direction? What if you had a blurry image and wanted to reconstruct the\n",
    "original, clear image? This *reverse* operation is what the adjoint operator aims to approximate.\n",
    "\n",
    "In mathematical terms, the adjoint operator is formally defined to be the dual of the forward operator with respect to a\n",
    "particular inner product space. It's often considered as a kind of *inverse* to the forward operator. However, it is\n",
    "important to note that it is not necessarily the true inverse. Rather, it can be viewed as a linear approximation to\n",
    "reverse the forward operator's action.\n",
    "\n",
    "In computational imaging and inverse problems, the adjoint operator is frequently used in iterative algorithms to\n",
    "reconstruct the original data from measurements. The forward operator models the measurement process, and the adjoint\n",
    "helps in the reconstruction process.\n",
    "\n",
    "So, when you see the term \"adjoint,\" you can think of it as the backward counterpart to your forward operator, designed\n",
    "to undo or approximate the undoing of whatever your forward operator did. \n",
    "\n",
    "Computing the adjoint of a complex forward operator can be challenging, especially in fields like tomography where even\n",
    "well-established implementations can have mismatched adjoints for the Radon transform. The mismatch often arises due to\n",
    "approximations or discretizations that make the adjoint non-trivial to compute directly.\n",
    "\n",
    "### Divide and Conquer Strategy\n",
    "\n",
    "A useful approach to tackle this issue is to divide and conquer: break down the complex operator into a chain of\n",
    "simpler, well-understood operators for which the adjoints are known or easily computed. Once you have this chain, you\n",
    "can leverage the properties of adjoint operators to calculate the adjoint of the entire composition. Mathematically, if\n",
    "you have operators $A$ and $B$, the adjoint of $AB$ is $B^* A^*$, where $^*$ denotes the adjoint.\n",
    "\n",
    "Pyxu excels in this context, thanks to its operator algebra logic. It allows you to compose operators easily, and when\n",
    "you ask for the adjoint, Pyxu will automatically compute it for you by leveraging the adjoint properties and the chain\n",
    "of simpler operators you've defined. In effect, Pyxu performs the heavy lifting, letting you focus on defining the core\n",
    "aspects of your operators.\n",
    "\n",
    "So, instead of getting bogged down with the mathematical intricacies of calculating a complex adjoint operator, you can\n",
    "use Pyxu to implement your forward operator as a chain of simpler ones. Pyxu takes care of the rest, giving you a\n",
    "powerful, yet computationally efficient, way to tackle challenging problems in computational imaging. Divide and\n",
    "conquer! üõ†Ô∏èüîç\n",
    "\n",
    "### Quality Assurance for Adjoint Operators in Pyxu\n",
    "\n",
    "Pyxu goes the extra mile to ensure the accuracy and integrity of the adjoint operators it ships. Our comprehensive test\n",
    "suite rigorously validates the correctness of adjoint implementations for every operator. This gives you the confidence\n",
    "that you are indeed working with mathematically valid adjoints, offering a robust foundation for your computational\n",
    "imaging projects. üõ°Ô∏èüîç\n",
    "\n",
    "## Additional Features of LinOp: Batteries Included \n",
    "\n",
    "Pyxu's `LinOp()`[üîó](../api/abc.html#pyxu.abc.LinOp) is designed to be as versatile and useful as possible for\n",
    "computational imaging. Here are some of the extra capabilities baked right into the class to facilitate both simple and\n",
    "complex tasks:\n",
    "\n",
    "- **Lipschitz Constant**: Knowing the Lipschitz constant of an operator is invaluable for optimization routines. If you\n",
    "  know it, you can directly store it in the `lipschitz`[üîó](../api/abc.html#pyxu.abc.Map.lipschitz) property. Don't know\n",
    "  it? No problem! The `estimate_lipschitz()`[üîó](../api/abc.html#pyxu.abc.Map.estimate_lipschitz) method will compute it\n",
    "  for you, offering various methods to balance accuracy and execution time.\n",
    "\n",
    "  ```python\n",
    "  my_operator.lipschitz = my_operator.estimate_lipschitz()\n",
    "  ```\n",
    "\n",
    "- **Singular Value Decomposition**: Ever needed to compute the singular values of your operator? Use the\n",
    "  `svdvals()`[üîó](../api/abc.html#pyxu.abc.LinOp.svdvals) method which employs the Arnoldi algorithm to get the job done\n",
    "  without breaking the matrix-free paradigm.\n",
    "\n",
    "- **Trace of Operator**: Sometimes you may need to compute the trace of an operator. The\n",
    "  `trace()`[üîó](../api/abc.html#pyxu.abc.SquareOp.trace) method uses a stochastic approximation with the Hutch++\n",
    "  algorithm to efficiently estimate it.\n",
    "\n",
    "- **Pseudo-Inverse**: If you're looking to solve an inverse problem, the\n",
    "  `pinv()`[üîó](../api/abc.html#pyxu.abc.LinOp.pinv) method helps you evaluate the pseudo-inverse by solving the\n",
    "  (dampened) normal equations, making it the most straightforward way to reverse your operations.\n",
    "\n",
    "  ```python\n",
    "  pseudo_inv_result = my_operator.pinv(my_array, damp=value)\n",
    "  ```\n",
    "\n",
    "- **Support for Explicit Matrices**: Though `LinOp()`[üîó](../api/abc.html#pyxu.abc.LinOp) is primarily built for\n",
    "  matrix-free operations, you are not restricted to them. If you prefer, you can work with explicit matrices using the\n",
    "  `from_array()`[üîó](../api/abc.html#pyxu.abc.LinOp.from_array) method, supporting a variety of array formats including\n",
    "  `numpy`, `cupy`, and `dask`.\n",
    "\n",
    "  ```python\n",
    "  op = LinOp.from_array(np.ones((N, N)))\n",
    "  ```\n",
    "\n",
    "With all these tools and methods at your disposal, `LinOp()`[üîó](../api/abc.html#pyxu.abc.LinOp) aims to be your go-to\n",
    "solution for implementing linear operators in computational imaging tasks.\n",
    "\n",
    "## DiffMap for Non-linear Forward Operators \n",
    "\n",
    "Linear models are often an idealized representation. In some situations forward operators are inherently non-linear.\n",
    "Take phase retrieval or some forms of tomography as examples: these are scenarios where the relationship between the\n",
    "object and its measurements isn't just a linear transformation. For such cases, Pyxu provides the\n",
    "`DiffMap()`[üîó](../api/abc.html#pyxu.abc.DiffMap) class to implement these non-linear forward operators.\n",
    "\n",
    "### Core Methods\n",
    "\n",
    "- `apply()`[üîó](../api/abc.html#pyxu.abc.Map.apply): This method is where you'll implement your forward model. Just like\n",
    "  in `LinOp()`[üîó](../api/abc.html#pyxu.abc.LinOp), you override this function to define how your specific forward model\n",
    "  operates on an input array.\n",
    "\n",
    "- `jacobian()`[üîó](../api/abc.html#pyxu.abc.DiffMap.jacobian): In calculus, the Jacobian matrix represents the best\n",
    "  linear approximation to a function at a given point. For a function $\\mathbf{f}: \\mathbb{R}^{M} \\to \\mathbb{R}^{N}$,\n",
    "  the Jacobian $\\mathbf{J}_{\\mathbf{f}}(\\mathbf{x})$ at a point $\\mathbf{x}$ is an $N \\times M$ matrix where the entry\n",
    "  $(i, j)$ is $\\frac{\\partial f_{i}}{\\partial x_{j}}$. When the forward operator is non-linear, knowing the Jacobian is\n",
    "  crucial for optimization algorithms.\n",
    "\n",
    "The `jacobian()` method allows you to evaluate (implicitly) this matrix, giving you the flexibility to implement the\n",
    "best way to approximate your non-linear operator linearly at a specific point.\n",
    "\n",
    "### A Nod to Deep Learning üöÄ\n",
    "\n",
    "When the Jacobian is unknown or too complex to derive analytically, techniques from deep learning come in handy.\n",
    "Libraries like JAX or PyTorch offer automatic differentiation techniques (autograd) to compute derivatives, which you\n",
    "can use to obtain the Jacobian efficiently in matrix-free fashion.\n",
    "\n",
    "In Pyxu, methods `from_torch`[üîó](../api/operator.interop.html#pyxu.operator.interop.from_torch) and\n",
    "`from_jax`[üîó](../api/operator.interop.html#pyxu.operator.interop.from_jax) allow you to define operators whose\n",
    "Jacobians are computed via autograd.  This means you can utilize the power of these deep learning libraries to implement\n",
    "non-linear operators seamlessly.\n",
    "\n",
    "With `DiffMap()`[üîó](../api/abc.html#pyxu.abc.DiffMap), you're well-equipped to tackle the challenges posed by\n",
    "non-linear forward operators in computational imaging.\n",
    "\n",
    "## Wrapping Up\n",
    "\n",
    "Pyxu offers a robust and scalable framework for computational imaging professionals to deal with both linear and\n",
    "non-linear forward operators. With the abstraction provided by classes like\n",
    "`LinOp()`[üîó](../api/abc.html#pyxu.abc.LinOp) and `DiffMap()`[üîó](../api/abc.html#pyxu.abc.DiffMap), you can focus on\n",
    "the problem you're solving rather than computational limitations. These classes are meant to be subclassed to tailor to\n",
    "your specific problem domain. Happy coding! üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "rise": {
   "footer": "Matthieu Simeoni, EPFL Hub for Advanced Image Reconstruction (AIR)",
   "progress": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
